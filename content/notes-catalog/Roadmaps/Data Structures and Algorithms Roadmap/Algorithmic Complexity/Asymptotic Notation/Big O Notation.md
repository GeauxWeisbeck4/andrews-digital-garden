---
id: 01JADKV667FXS8H9BXNGW6T12X
title: Big O Notation
modified: 2024-10-17T12:11:18-04:00
tags:
  - runtimes
  - dsa
  - algorithms
  - algorithmic-complexity
  - programming
  - roadmaps
---
# Big O Notation

“Big O” notation, officially known as O-notation, is used in computer science to describe the performance or complexity of an algorithm. Specifically, it provides an upper bound on the time complexity, describing the worst-case scenario. Thus, it gives an upper limit on the time taken for an algorithm to complete based on the size of the input. The notation is expressed as O(f(n)), where f(n) is a function that measures the largest count of steps that an algorithm could possibly take to solve a problem of size n. For instance, O(n) denotes a linear relationship between the time taken and the input size, while O(1) signifies constant time complexity, i.e., the time taken is independent of input size. Remember, Big O notation is only an approximation meant to describe the scaling of the algorithm and not the exact time taken.

Learn more from the following links:

Free Resources

---

- [videoIntroduction to Big O Notation and Time Complexity](https://www.youtube.com/watch?v=D6xkbGLQesk)
- [videoBig-O Notation](https://www.youtube.com/watch?v=BgLTDT03QtU)